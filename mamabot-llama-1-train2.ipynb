{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":81878,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":68806,"modelId":91102}],"dockerImageVersionId":30747,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Install necessary libraries\n!pip install -q datasets huggingface_hub\n!pip install -q -U peft\n!pip install -q -U trl\n!pip install -q git+https://github.com/huggingface/accelerate.git\n!pip install -q git+https://github.com/huggingface/transformers.git\n!pip install -q -U bitsandbytes\n!pip install -q wandb","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Import libraries\nfrom datasets import Dataset, load_dataset\nimport pandas as pd\nfrom transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,Trainer,\n    BitsAndBytesConfig,\n    HfArgumentParser,\n    TrainingArguments,\n    pipeline,AutoConfig,\n    logging,\n)\nfrom peft import (\n    LoraConfig,\n    PeftModel,\n    prepare_model_for_kbit_training,\n    get_peft_model,\n)\nimport os\nimport time\nimport torch\nfrom datasets import Dataset\nfrom huggingface_hub import notebook_login, HfFolder\nfrom trl import SFTTrainer,setup_chat_format, SFTConfig\nfrom peft import LoraConfig, PeftModel, prepare_model_for_kbit_training","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"compute_dtype = torch.bfloat16\n\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=compute_dtype,\n    bnb_4bit_use_double_quant=False\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Paths to models\nbase_model_path = \"/kaggle/input/llama-3.1/transformers/8b/1\"\nadapter_model_path = \"Helpmum-Personal/mamabot-llama-adapter\"\n\n\nbase_model = AutoModelForCausalLM.from_pretrained(base_model_path, quantization_config=bnb_config)\nprint(\"Base model loaded successfully.\")\n\n# Load the tokenizer\ntokenizer = AutoTokenizer.from_pretrained(base_model_path)\n\n# Resize token embeddings\nbase_model.resize_token_embeddings(len(tokenizer))\nprint(\"Token embeddings resized successfully.\")\n\n# Load the adapter model with ignore_mismatched_sizes set to True\nmodel = PeftModel.from_pretrained(base_model, adapter_model_path, ignore_mismatched_sizes=True)\n\n# Move model to GPU\nmodel = model.to(\"cuda\")\nmodel.eval()\nprint(\"Model moved to GPU and set to evaluation mode successfully.\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model=model.merge_and_unload()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from huggingface_hub import HfApi, HfFolder\n\nhf_token = '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'\n\n# Set the token\nHfFolder.save_token(hf_token)  \n\napi = HfApi()\nwhoami = api.whoami(token=hf_token)\nprint(f\"Logged in as: {whoami['name']}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"repo_id = \"HelpMum-Personal/mamabot-llama-1\"\nmodel.push_to_hub(repo_id = repo_id, tokenizer = tokenizer, token=hf_token)\ntokenizer.push_to_hub(repo_id = repo_id, token=hf_token)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}